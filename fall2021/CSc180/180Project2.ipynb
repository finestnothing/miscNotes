{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95124d88-2017-4439-87e2-12e9846499b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import csv\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from matplotlib.pyplot import figure, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2447ee0a-f9c8-496f-a188-30b98df5e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Reviews tsv file with business_id, stars, and text\n",
    "outfile=open('review_stars.tsv', 'w')\n",
    "sfile=csv.writer(outfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "sfile.writerow(['business_id','stars','text']) #column titles\n",
    "\n",
    "with open('yelp_dataset/yelp_academic_dataset_review.json', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        row=json.loads(line)\n",
    "        sfile.writerow([row['business_id'],row['stars'],(row['text']).encode('utf-8')])\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2131ff81-c675-4859-98d9-a6160a9c4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make business tsv file\n",
    "outfile=open('business_stars.tsv', 'w')\n",
    "sfile=csv.writer(outfile,delimiter='\\t',quoting=csv.QUOTE_MINIMAL)\n",
    "sfile.writerow(['business_id','categories', 'name'])\n",
    "\n",
    "with open('yelp_dataset/yelp_academic_dataset_business.json', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        if(row['review_count']>=20):\n",
    "            sfile.writerow([row['business_id'], row['categories'], (row['name']).encode('utf-8')])\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526a096c-2cea-49cd-8590-6f6b8b3f65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews=pd.read_csv('review_stars.tsv', sep='\\t')\n",
    "df_business=pd.read_csv('business_stars.tsv', sep='\\t')\n",
    "df_merged=pd.merge(df_reviews, df_business, on='business_id')\n",
    "df_merged.to_csv('merged_df.tsv',sep='\\t')\n",
    "del df_reviews\n",
    "del df_business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e784b-e1db-48ce-8038-6a8367db6e33",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32dd5f73-f1fa-455c-8c08-e61e656805ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged=pd.read_csv('merged_df.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f51671e9-1645-4089-84a4-0752ae910615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limiting number of reviews due to size\n",
    "numEntries = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a4f831-8433-4872-9589-7a0d6d8f2374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = sk_text.TfidfVectorizer(\n",
    "                                    max_features=1000,\n",
    "                                    max_df=1000,\n",
    "                                    min_df=1)\n",
    "matrix=vectorizer.fit_transform(df_merged.text[0:numEntries])\n",
    "tfidf_data=matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa94e935-3dcc-4971-873c-784c41852aa5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tfidf_data\n",
    "y = df_merged.stars[0:numEntries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4a4ea73-1ecb-4682-8dec-6e56911c4336",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 - 0s - loss: 3.7936 - val_loss: 1.0326\n",
      "Epoch 2/10\n",
      "235/235 - 0s - loss: 0.8299 - val_loss: 0.8410\n",
      "Epoch 3/10\n",
      "235/235 - 0s - loss: 0.7051 - val_loss: 0.8111\n",
      "Epoch 4/10\n",
      "235/235 - 0s - loss: 0.6607 - val_loss: 0.7930\n",
      "Epoch 5/10\n",
      "235/235 - 0s - loss: 0.6307 - val_loss: 0.7898\n",
      "Epoch 6/10\n",
      "235/235 - 0s - loss: 0.5910 - val_loss: 0.7893\n",
      "Epoch 7/10\n",
      "235/235 - 0s - loss: 0.5302 - val_loss: 0.7825\n",
      "Epoch 8/10\n",
      "235/235 - 0s - loss: 0.4258 - val_loss: 0.7887\n",
      "Epoch 9/10\n",
      "235/235 - 0s - loss: 0.2926 - val_loss: 0.8067\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=32)\n",
    "\n",
    "model.add(Dense(100, input_dim=x.shape[1], activation='relu')) \n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=2, verbose=1, mode='auto')  \n",
    "\n",
    "#model.fit(x_train, y_train, validation_data=(x_test,y_test), callbacks=[monitor], verbose=2, epochs=1000)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "model.fit(x_train, y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=2,epochs=100)\n",
    "\n",
    "model.load_weights('dnn/best_weights.hdf5') # load weights from best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a3b56d-c066-4007-a404-6651f2a986d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19657/495196763.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9421fc81-66fd-4ac1-8e4e-16499f0bcf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE score: 0.8845655210421726\n"
     ]
    }
   ],
   "source": [
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final RMSE score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef4bc5-268d-43e7-95e9-f1eb530a5d49",
   "metadata": {},
   "source": [
    "# Random Business Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50f1885b-2086-474d-818d-7e313c480b46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2476610/2880048907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                     \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     min_df=1)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtfidf_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \"\"\"\n\u001b[1;32m   1845\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1846\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1203\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer = sk_text.TfidfVectorizer(\n",
    "                                    max_features=1000,\n",
    "                                    max_df=1000,\n",
    "                                    min_df=1)\n",
    "matrix=vectorizer.fit_transform(df_merged.groupby(text[:])\n",
    "tfidf_data=matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db5aad34-af49-45d0-9bc0-9a5704720350",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tfidf_data\n",
    "y = df_merged.stars[0:numEntries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497327a3-1a76-4b98-972b-9e72cb7e4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
